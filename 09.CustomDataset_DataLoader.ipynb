{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN9MjhF0fBIahrskk+5mOx4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1.Dataset/DataLoader class"],"metadata":{"id":"aKywo8gSXcwh"}},{"cell_type":"markdown","source":["# 1.CustomDataset 정의"],"metadata":{"id":"r0WcRCjfdUUS"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","# Dataset class를 상속받는 CustomDataset class 정의\n","# Dataset class를 상속받았기 때문에\n","# __init__(), __getitem__(), __len__() 를 재정의(Overriding)해야 함\n","class CustomDataset(Dataset):\n","\n","  def __init__(self, x_train, y_train):\n","    # 입력 data(input feature)와 정답(label)을 저장\n","    self.x_train = x_train\n","    self.y_train = y_train\n","\n","  def __getitem__(self, index):\n","    # index에 해당하는 요소(입력 data, 정답)를 반환\n","    return self.x_train[index], self.y_train[index]\n","\n","  def __len__(self):\n","    # 입력 data의 크기(size)를 반환\n","    return self.x_train.shape[0]"],"metadata":{"id":"mbFU5A9LYC7K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.instance 생성"],"metadata":{"id":"4U7U0zm2dbgQ"}},{"cell_type":"code","source":["import torch\n","\n","x_train = torch.Tensor([1, 2, 3, 4, 5, 6]).view(6, 1)\n","y_train = torch.Tensor([3, 4, 5, 6, 7, 8]).view(6, 1)\n","\n","print(x_train)\n","print(y_train)\n","\n","# CustomDataset의 instance 생성\n","dataset = CustomDataset(x_train, y_train)\n","# CustomDataset을 내부 변수로 가지는 DataLoader의 instance 생성\n","train_loader = DataLoader(dataset=dataset, batch_size=3, shuffle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FMX8VKnIY4ZS","executionInfo":{"status":"ok","timestamp":1729051571958,"user_tz":-540,"elapsed":9,"user":{"displayName":"서철규","userId":"08589483314836926095"}},"outputId":"83c6b400-9575-44c0-a387-d105b1813c7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.],\n","        [2.],\n","        [3.],\n","        [4.],\n","        [5.],\n","        [6.]])\n","tensor([[3.],\n","        [4.],\n","        [5.],\n","        [6.],\n","        [7.],\n","        [8.]])\n"]}]},{"cell_type":"markdown","source":["# 3.사용 예"],"metadata":{"id":"MZJHLTokdiO4"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class SampleLinearRegressionModel(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.linear_stack = nn.Sequential(\n","        nn.Linear(1, 1)\n","    )\n","\n","  def forward(self, x):\n","    return self.linear_stack(x)"],"metadata":{"id":"kQJYZ2jTfgPJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","model = SampleLinearRegressionModel()\n","criterion = nn.MSELoss()\n","optimizer = optim.SGD(model.parameters(), lr=1e-4)\n","\n","epochs = 2\n","for epoch in range(epochs):\n","  # 전체 data는 6개, batch_sixe=3 이므로 6 / 3 = 2번 반복\n","  # epoch 한 번당 2번 반복한다는 의미\n","  for idx, batch_data in enumerate(train_loader):\n","    # 3개씩의 입력/정답 data\n","    x_train_batch, y_train_batch = batch_data\n","    # 3개의 계산값을 return\n","    output_batch = model(x_train_batch)\n","\n","    print(f'len(train_loader) = {len(train_loader)}')\n","    print(\"==========================================================\")\n","    print(f'epoch = {epoch}, batch_idx = {idx}')\n","    print(f'x_train_batch: len = {len(x_train_batch)}\\n{x_train_batch}')\n","    print(f'y_train_batch: len = {len(y_train_batch)}\\n{y_train_batch}')\n","    print(f'output_batch: len = {len(output_batch)}\\n{output_batch}')\n","\n","    loss = criterion(output_batch, y_train_batch)\n","    print(f'loss = {loss}')\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uBkJOvBhdIVJ","executionInfo":{"status":"ok","timestamp":1729052172270,"user_tz":-540,"elapsed":404,"user":{"displayName":"서철규","userId":"08589483314836926095"}},"outputId":"89104d84-fd17-4e01-f773-84ec5c593a8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["len(train_loader) = 2\n","==========================================================\n","epoch = 0, batch_idx = 0\n","x_train_batch: len = 3\n","tensor([[1.],\n","        [3.],\n","        [5.]])\n","y_train_batch: len = 3\n","tensor([[3.],\n","        [5.],\n","        [7.]])\n","output_batch: len = 3\n","tensor([[0.3994],\n","        [1.8577],\n","        [3.3160]], grad_fn=<AddmmBackward0>)\n","loss = 10.069598197937012\n","len(train_loader) = 2\n","==========================================================\n","epoch = 0, batch_idx = 1\n","x_train_batch: len = 3\n","tensor([[4.],\n","        [6.],\n","        [2.]])\n","y_train_batch: len = 3\n","tensor([[6.],\n","        [8.],\n","        [4.]])\n","output_batch: len = 3\n","tensor([[2.5956],\n","        [4.0580],\n","        [1.1332]], grad_fn=<AddmmBackward0>)\n","loss = 11.782561302185059\n","len(train_loader) = 2\n","==========================================================\n","epoch = 1, batch_idx = 0\n","x_train_batch: len = 3\n","tensor([[4.],\n","        [1.],\n","        [5.]])\n","y_train_batch: len = 3\n","tensor([[6.],\n","        [3.],\n","        [7.]])\n","output_batch: len = 3\n","tensor([[2.6078],\n","        [0.4056],\n","        [3.3418]], grad_fn=<AddmmBackward0>)\n","loss = 10.540165901184082\n","len(train_loader) = 2\n","==========================================================\n","epoch = 1, batch_idx = 1\n","x_train_batch: len = 3\n","tensor([[6.],\n","        [3.],\n","        [2.]])\n","y_train_batch: len = 3\n","tensor([[8.],\n","        [5.],\n","        [4.]])\n","output_batch: len = 3\n","tensor([[4.0903],\n","        [1.8812],\n","        [1.1449]], grad_fn=<AddmmBackward0>)\n","loss = 11.054710388183594\n"]}]}]}